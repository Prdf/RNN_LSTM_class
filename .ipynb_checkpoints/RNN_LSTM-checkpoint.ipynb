{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Recurrent Neural Networks - Long Short Term Memory</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was seen beforehand in previous courses, Neural Networks offer a very comprehensive and efficient solution to classification problems. However could it also be applied to other problems such as prediction and time series forecasting? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Translate, Apple's Siri, ... What do these applications have in common? They rely on specific Neural Networks called Recurrent Neural Networks. The core difference with what we have been seen before is the ability of RNNs to \"remember\" informations from previous chunk of the network. These networks were introduced by David Rumelhart's work in 1986.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the theoritical explanation, let's illustrate this idea with a graphical representation.\n",
    "\n",
    "<img src=\"Pics/RNN-rolled.png\" alt=\"RNN-rolled.PNG\" style=\"width: 100px;\"/>\n",
    "\n",
    "This \"rolled\" representation of the RNN shows us an input $x_t$ in the network that then outputs an $h_t$ called the hidden state. \n",
    "\n",
    "We can also see that there is a loop on the network that can be unrolled (see below) to represent the loop mechanism of a RNN. \n",
    "\n",
    "<img src=\"Pics/RNN-unrolled.png\" alt=\"RNN-unrolled.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "We see here that the input is in reality made out of several inputs for each step $1$ to $t$. The network feeds itself with previous values it computed to improve the results its predictions. The most simple form of RNN feeds itself its outputs as inputs for instance after a tanh activation (see: https://keras.io/layers/recurrent/ - SimpleRNN).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before the network relies on the hidden state $h_i$ which is the \"memory\" of the network, an information about what the network has seen beforehand.\n",
    "\n",
    "To compute the hidden state, several methods can be used, for instance with the SimpleRNN layer in Keras, the output can become the new hidden state. \n",
    "\n",
    "A more general method is the concatenation of the previous hidden state and the input into a vector. This vector then goes through a $tanh$ activation to avoid a uncontrollable growth of the values in the network and the output of this activation is the new hidden state $h_t$.\n",
    "\n",
    "To summarize it, here is a visual representation of the process: \n",
    "<img src=\"Pics/RNN_resume.gif\" alt=\"RNN_resume.gif\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a simple RNN  we will use the dataset and the example of the following article. <br>  \n",
    "<center><b>Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras </b></center> <br>\n",
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by plotting the data in order to grasp the problem at hand.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset = pandas.read_csv('data/international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "plt.plot(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first insight on our data shows us that the sales seem to follow a cyclic pattern and as such a NN with the ability to \"remember\" what came before could be a good approah to this problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce the create_dataset function that we will use for our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the create_dataset function given beforehand, reshape test and train into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = #...\n",
    "testX, testY = #...\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = #...\n",
    "testX = #..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load solutions/code1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen here, the results are close to reality, thus illustrating the efficiency of RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The vanishing gradient issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However RNNs have a very impairing issue which is the vanishing of gradient during the backpropagation phase of the network. Some gradients converge very quickly to 0 preventing any modification in the network's weigths and thus limiting the network efficiency and accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory of a RNN is very limited and will forget earlier data while it will propagate trough the network. This memory is only short-term oriented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Before starting the second part of the notebook, feel free to take a short break\n",
    "<img src=\"Pics/Slacking.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compose with RNNs short-comings, a new cell was designed to improve its capacity, while exploiting its \"memory\" capacity. These are called LSTM. They were introduced in 1997 by Hochreiter and Schmihuber in the article, \"Long Short-term Memory\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick guide on LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LTSM cells rely on the same idea as RNNs as it propagates information in the network. The operations in the cell are more elaborate than those of an RNN. We shall go through them step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/LSTM_resume.png\" title=\"A LSTM cell\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell is built around three main gates, which are activation functions. RNNs relied on $tanh$ mostly, but LSTM cells add sigmoid activation to allow the network to forget information and not keep all of it like with a RNN.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forget gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first gate decides what information to keep. After a concatenation of the current input and the previous hidden state into a vector, this information wil be passed through a sigmoid activation to decide what is to be kept or forgotten.\n",
    "<img src=\"Pics/forget_gate.gif\" alt=\"forget_gate.gif\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input gate is meant to prepare the update of the cell state and is made of two activation functions in parallel that are passed through by the previously concatenated vector.\n",
    "\n",
    "The sigmoid activation is here to choose what information is to be kept and the $tanh$ activation help regulating the network by keeping the values it gets between 1 and -1 (thus avoid the possible escalation of some values)\n",
    "\n",
    "The outputs of these two activations are then multiplied and the result will be used to update our cell state\n",
    "\n",
    "<img src=\"Pics/input_gate.gif\" alt=\"RNN_resume.gif\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this step to update the previous cell state by first multiply it by the forget gate output to drop irrelevant values and we then add the output of the input gate to create the new cell state\n",
    "<img src=\"Pics/cell_state.gif\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last gate is the output gate where the new hidden state is computed based on its previous hidden state and the new cell state. \n",
    "The cell state is squished through a $tanh$ activation and the previously concatenated vector is passed through a sigmoid activation to forget irrelevant information. \n",
    "\n",
    "The outputs of both gates are multiplied to create the new hidden state.\n",
    "\n",
    "<img src=\"Pics/output_gate.gif\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><center>Now please try to complete the following code which summarizes the process inside a LSTM cell\n",
    "\n",
    "\n",
    "<center>This is a pseudo-code exercise, no need to run the cell afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTMcell(prev_ht, prev_ct,input):\n",
    "    combine = #...\n",
    "    forget_t = forget_layer(#...)\n",
    "    candidate = candidate_layer(#...)\n",
    "    input_t = input_layer(#...)\n",
    "    c_t = #...\n",
    "    output_t = output_layer(#...)\n",
    "    h_t = #...\n",
    "    return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/code2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to implement LSTM on our previous example to compare their efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape test and train into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our results were improved by simply replacing the RNN cells by LSTM cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To go further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about RNN and LSTM uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the introduction, these cells can be used for prediction and classification, here are two articles about these uses that could come in handy later or that you could explore if you want to see more applications.\n",
    "\n",
    "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/ : a text generation model trained on Lewis Caroll's <i>\"Alice in Wonderland\"</i> by Jason Brownlee \n",
    "\n",
    "https://towardsdatascience.com/forecasting-air-pollution-with-recurrent-neural-networks-ffb095763a5c : Air quality forecast by Bert Carremans. It is highly interesting as it compares the efficiency of RNNs, simple LSTMs and more complex forms of LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gated Recurrent Unit</b> (GRU): These structures are a variation of LSTMs, where the two first gates are combined in a single update gate. It was introduced by Cho, et al. (2014). \n",
    "<b> Benchmarks </b> The benchmark of the efficiency of these networks made by Greff, et al. (2015) is a relevant article \n",
    "\n",
    "RNNs and LSTMs have be a hot topic for some time now and new results and studies about them are published often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example source:<br>\n",
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "\n",
    "Guides on RNN and LSTM:<br>\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "https://skymind.ai/wiki/lstm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
